{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from scipy.stats import pearsonr\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import Predictive\n",
    "from pyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import creep data\n",
    "creep_df = pd.read_csv('SS316_PI_dataset.csv')\n",
    "creep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(PyroModule):\n",
    "    def __init__(self, in_dim=21, out_dim=1, hid_dim=10, n_hid_layers=5, prior_scale=5.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = nn.Tanh()  # could also be ReLU or LeakyReLU\n",
    "        assert in_dim > 0 and out_dim > 0 and hid_dim > 0 and n_hid_layers > 0  # make sure the dimensions are valid\n",
    "\n",
    "        # Define the layer sizes and the PyroModule layer list\n",
    "        self.layer_sizes = [in_dim] + n_hid_layers * [hid_dim] + [out_dim]\n",
    "        layer_list = [PyroModule[nn.Linear](self.layer_sizes[idx - 1], self.layer_sizes[idx]) for idx in\n",
    "                      range(1, len(self.layer_sizes))]\n",
    "        self.layers = PyroModule[torch.nn.ModuleList](layer_list)\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            layer.weight = PyroSample(dist.Normal(0., prior_scale * np.sqrt(2 / self.layer_sizes[layer_idx])).expand(\n",
    "                [self.layer_sizes[layer_idx + 1], self.layer_sizes[layer_idx]]).to_event(2))\n",
    "            layer.bias = PyroSample(dist.Normal(0., prior_scale).expand([self.layer_sizes[layer_idx + 1]]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # x = x.reshape(-1, 1)\n",
    "        x = self.activation(self.layers[0](x))  # input --> hidden\n",
    "        for layer in self.layers[1:-1]:\n",
    "            x = self.activation(layer(x))  # hidden --> hidden\n",
    "        mu = self.layers[-1](x).squeeze()  # hidden --> output\n",
    "        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))  # infer the response noise\n",
    "\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mu, sigma * sigma), obs=y)\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_state = 123\n",
    "test_size = 0.6\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(np.array(creep_df.iloc[:, 0:21]), np.array(creep_df.iloc[:,21]), shuffle=True, test_size=test_size, random_state=rm_state)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(y))\n",
    "\n",
    "train_ratio = 0.1\n",
    "X_train, _, y_train, _, idx_train, idx_pool = train_test_split(X, y, idx, train_size=train_ratio, shuffle=True, random_state=rm_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_variance = []\n",
    "r2_variance = []\n",
    "rmse_variance = []\n",
    "mae_variance = []\n",
    "num_training_data=[]\n",
    "\n",
    "pcc_random = []\n",
    "r2_random = []\n",
    "num_iteration = []\n",
    "\n",
    "X_train_var = X_train\n",
    "X_train_ran = X_train\n",
    "y_train_var = y_train\n",
    "y_train_ran = y_train\n",
    "idx_pool_var = idx_pool\n",
    "idx_pool_ran = idx_pool\n",
    "idx_train_var = idx_train\n",
    "idx_train_ran = idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "test_list = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print(f\"Performing iteration : {i}\")\n",
    "\n",
    "    if i != 0:\n",
    "        # find 10 data points with the highest variance\n",
    "        \n",
    "        q_points_var = np.argpartition(y_pred_unc_pool_var, -10)[-10:]\n",
    "        # indices of those points in idx_pool\n",
    "        idx_pool_train_var = idx_pool_var[q_points_var]\n",
    "\n",
    "        idx_train_var = np.append(idx_train_var, idx_pool_train_var)\n",
    "        idx_pool_var = np.delete(idx_pool_var, q_points_var)\n",
    "        \n",
    "        X_train_var = X[idx_train_var]\n",
    "        y_train_var = y[idx_train_var]\n",
    "       \n",
    "    print(f\"Number of training data with variance: {len(idx_train_var)}\")\n",
    "    print(f\"Number of pooling data with variance: {len(idx_pool_var)}\")\n",
    "    \n",
    "    num_training_data.append(len(idx_train_var))\n",
    "    \n",
    "    model = BNN(hid_dim=10, n_hid_layers=3, prior_scale=1)\n",
    "    nuts_kernel = NUTS(model, jit_compile=False)\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=100)\n",
    "    mcmc.run(torch.Tensor(X_train_var), torch.Tensor(y_train_var))\n",
    "    predictive_var = Predictive(model=model, posterior_samples=mcmc.get_samples())\n",
    "    preds_var = predictive_var(torch.Tensor(X_test))\n",
    "\n",
    "    # mean and standard deviation of the test dataset\n",
    "    y_pred_test_var = preds_var['obs'].T.detach().numpy().mean(axis=1)\n",
    "    y_std_test_var = preds_var['obs'].T.detach().numpy().std(axis=1)\n",
    "\n",
    "    preds_pool_var = predictive_var(torch.Tensor(X[idx_pool_var]))\n",
    "    y_pred_unc_pool_var = preds_pool_var['obs'].T.detach().numpy().std(axis=1)\n",
    "\n",
    "    print('PCC_test variance', pearsonr(np.squeeze(y_test), np.squeeze(y_pred_test_var))[0])\n",
    "    print('R2_test variance', r2_score(np.squeeze(y_test), np.squeeze(y_pred_test_var)))\n",
    "    print('RMSE', np.sqrt(mean_squared_error(y_test, y_pred_test_var)))\n",
    "    print('MAE', np.mean(abs(y_test - y_pred_test_var)))\n",
    "\n",
    "    pcc_variance.append(pearsonr(np.squeeze(y_test), np.squeeze(y_pred_test_var))[0])\n",
    "    r2_variance.append(r2_score(np.squeeze(y_test), np.squeeze(y_pred_test_var)))\n",
    "    rmse_variance.append(np.sqrt(mean_squared_error(y_test, y_pred_test_var)))\n",
    "    mae_variance.append(np.mean(abs(y_test - y_pred_test_var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('AL_BNN_MCMC_PI.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump({'train_numbs':num_training_data, 'pcc':pcc_variance,'r2':r2_variance, 'rsme': rmse_variance, 'mae': mae_variance}, f)\n",
    "    f.close()\n",
    "\n",
    "pkl_file = open('AL_BNN_MCMC_PI.pkl', 'rb')  \n",
    "test_ALGPR = pickle.load(pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
